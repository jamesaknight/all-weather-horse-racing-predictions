# Attempted Hyper-parameter optimisations

## Optimisers

| Stochastic Gradient Descent | Adam | RMSProp | ADAgrad | AdamW |
|:-:|:-:|:-:|:-:|:-:|
| lr = 0.001, momentum 0.6-0.9 |lr = 0.001| alpha 0.99| lr 0.01| decay 0.01|

## Loss function

Huber - delta 1
MSE